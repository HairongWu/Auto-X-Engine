# Auto-X Engine

Auto-X Engine is an open source deep learning training/inference framework that could be used for mobile, edge and cloud scenarios.

Auto-X Engine Server is an open source inference serving software that streamlines AI inferencing. It enables users to deploy AI models from Auto-X Engine. It supports inference across cloud, data center, edge and embedded devices on NVIDIA GPUs, x86 and ARM CPU, or AWS Inferentia. It also delivers optimized performance for many query types, including real time, batched, ensembles and audio/video streaming.

Auto-X Engine Lite written in pure C is a library for deploying deep learning models in ultra-low power devices.


## Model Pool

> **Note** The following models could be modified from the originial ones.
> We also provide guidelines and running code to customize and retrain the following models using your own data.

### Tiny Models for MCU (such as ESP32 and Arm Cortex-M)


### Medium Models for CPU (such as Arm Cortex-A and X86)


### Large-scale Models for CPU/GPU servers


## Reference

- [Triton Inference Server](https://github.com/triton-inference-server/server?tab=readme-ov-file)
- [MNN](https://github.com/alibaba/MNN)